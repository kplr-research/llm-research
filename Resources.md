# LLM Resources :

## Transformers Architecture :

### Videos :

- [Attention Mechanism Explained ](https://www.youtube.com/watch?v=PSs6nxngL6k&t=6s)
- [Attention is all you need (Transformer) - Model explanation (including math), Inference and Training](https://www.youtube.com/watch?v=bCz4OMemCcA)
- [Transformer Neural Networks, ChatGPT's foundation, Clearly Explained!!!](https://www.youtube.com/watch?v=zxQyTK8quyY&t=87s)
- [Word Embedding in PyTorch + Lightning](https://www.youtube.com/watch?v=Qf06XDYXCXI)
- [Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly Explained!!!](https://www.youtube.com/watch?v=bQ5BoolX9Ag)
### Papers , Articles :
- *
- 

### Codes :
- *
- 
## Hugging Face :

### Videos :
- *
- 
### Papers , Articles :
- *
- 
### Codes :
- *
- 

## Llama Architecture :

### Videos :

- [LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU](https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2874s)
- ..
- 
### Papers , Articles :
- *
- 
### Codes :
- *
- 
## Mistral Architecture :
### Videos :
- [Mistral / Mixtral Explained: Sliding Window Attention, Sparse Mixture of Experts, Rolling Buffer](https://www.youtube.com/watch?v=UiX8K-xBUpE)

### Papers , Articles :
- *
- 
### Codes :
- *
- 
