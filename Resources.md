# LLM Resources :

## Transformers Architecture :

### Videos :

- [Attention Mechanism Explained ](https://www.youtube.com/watch?v=PSs6nxngL6k&t=6s)
- [Attention is all you need (Transformer) - Model explanation (including math), Inference and Training](https://www.youtube.com/watch?v=bCz4OMemCcA)
- [Transformer Neural Networks, ChatGPT's foundation, Clearly Explained!!!](https://www.youtube.com/watch?v=zxQyTK8quyY&t=87s)
- [Word Embedding in PyTorch + Lightning](https://www.youtube.com/watch?v=Qf06XDYXCXI)
- [Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly Explained!!!](https://www.youtube.com/watch?v=bQ5BoolX9Ag)
### Papers , Articles :
- [Attention is all you need ](https://arxiv.org/abs/1706.03762)
- 

### Codes :
- *
- 
## Hugging Face :

### Videos :
- *
- 
### Papers , Articles :
- *
- 
### Codes :
- *
- 

## Llama Architecture :

### Videos :

- [LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU](https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2874s)
- ..
- 
### Papers , Articles :
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- 
### Codes :
- *
- 
## Mistral Architecture :
### Videos :
- [Mistral / Mixtral Explained: Sliding Window Attention, Sparse Mixture of Experts, Rolling Buffer](https://www.youtube.com/watch?v=UiX8K-xBUpE)

### Papers , Articles :
- [Mistral 7B](https://arxiv.org/abs/2310.06825)
- 
### Codes :
- *
- 
## Micro-Batch Pipeline Parallelism :
### Papers , Articles :
- [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism](https://arxiv.org/abs/2310.06825](https://arxiv.org/pdf/1811.06965.pdf%E2%80%8Barxiv.org)https://arxiv.org/pdf/1811.06965.pdf%E2%80%8Barxiv.org)
