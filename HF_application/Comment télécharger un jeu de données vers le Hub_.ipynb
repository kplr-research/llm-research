{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2d7ac0",
   "metadata": {
    "id": "wjTrAC9SVtFd"
   },
   "source": [
    "## Comment effectuer l'association de la m√©moire et le streaming ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02c724",
   "metadata": {
    "id": "MVeakzxeDCgm"
   },
   "outputs": [],
   "source": [
    "! pip install datasets transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8c60f",
   "metadata": {
    "id": "8Bz-fCh5DCgm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
    "large_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
    "size_gb = large_dataset.dataset_size / (1024 ** 3)\n",
    "print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d5e784",
   "metadata": {
    "id": "XGqMIAFRDCgm"
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ade1e",
   "metadata": {
    "id": "JIkV92E8DCgn"
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "code_snippet = \"\"\"batch_size = 1000\n",
    "\n",
    "for idx in range(0, len(large_dataset), batch_size):\n",
    "    _ = large_dataset[idx:idx + batch_size]\n",
    "\"\"\"\n",
    "\n",
    "time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())\n",
    "print(\n",
    "    f\"Iterated over {len(large_dataset)} examples (about {size_gb:.1f} GB) in \"\n",
    "    f\"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7897c",
   "metadata": {
    "id": "OvKt-9BXDCgn"
   },
   "outputs": [],
   "source": [
    "large_dataset_streamed = load_dataset(\n",
    "    \"json\", data_files=data_files, split=\"train\", streaming=True)\n",
    "\n",
    "next(iter(large_dataset_streamed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e64584",
   "metadata": {
    "id": "ikGv1CIxDCgn"
   },
   "outputs": [],
   "source": [
    "type(large_dataset_streamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c5cc6",
   "metadata": {
    "id": "ADBad4WhDCgn"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized_dataset = large_dataset_streamed.map(lambda x: tokenizer(x[\"text\"]))\n",
    "next(iter(tokenized_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187dd826",
   "metadata": {
    "id": "rn-xOQL6DCgo"
   },
   "outputs": [],
   "source": [
    "# Select the first 5 examples\n",
    "dataset_head = large_dataset_streamed.take(5)\n",
    "list(dataset_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027255fe",
   "metadata": {
    "id": "nRD85KpFDCgo"
   },
   "outputs": [],
   "source": [
    "# Skip the first 1,000 examples and include the rest in the training set\n",
    "train_dataset = large_dataset_streamed.skip(1000)\n",
    "# Take the first 1,000 examples for the validation set\n",
    "validation_dataset = large_dataset_streamed.take(1000)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
