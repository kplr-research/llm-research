{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab012da",
   "metadata": {
    "id": "6K58vGlCZfzf"
   },
   "source": [
    "## Comment construire un nouveau Tokenizer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ca9c5",
   "metadata": {
    "id": "914vDKiC-Emv"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90e79c",
   "metadata": {
    "id": "7FOqIrGu-Emv"
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers, processors, decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be75cb5c",
   "metadata": {
    "id": "PsyTAo2k-Emv"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6480d0",
   "metadata": {
    "id": "22Vf2zL9-Emw"
   },
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(Regex(r\"[\\p{Other}&&[^\\n\\t\\r]]\"), \"\"),\n",
    "        normalizers.Replace(Regex(r\"[\\s]\"), \" \"),\n",
    "        normalizers.Lowercase(),\n",
    "        normalizers.NFD(), normalizers.StripAccents()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e715b838",
   "metadata": {
    "id": "Ziunzlnr-Emw"
   },
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0444d",
   "metadata": {
    "id": "EbtPbiyK-Emw"
   },
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a609b",
   "metadata": {
    "id": "0EBfmCFi-Emx"
   },
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39015e0b",
   "metadata": {
    "id": "zInUk7iO-Emx"
   },
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655bc10",
   "metadata": {
    "id": "t3TS1SgR-Emx"
   },
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
