{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f38385f",
   "metadata": {
    "id": "8znwRUkRaQcI"
   },
   "source": [
    "## Comment faire le traitement des donn√©es pour la traduction ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c382e",
   "metadata": {
    "id": "7VWvKl3daX_L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309e295",
   "metadata": {
    "id": "N6Kh5fXfBCXs"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "\n",
    "def extract_languages(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    return {\"inputs\": inputs, \"targets\": targets}\n",
    "\n",
    "raw_datasets = raw_datasets.map(extract_languages, batched=True, remove_columns=[\"id\", \"translation\"])\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad8ae6",
   "metadata": {
    "id": "cllFBK0VBCXs"
   },
   "outputs": [],
   "source": [
    "print(raw_datasets[\"train\"][10])\n",
    "print(raw_datasets[\"train\"][11])\n",
    "print(raw_datasets[\"train\"][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a35763e",
   "metadata": {
    "id": "8uirofTXBCXt"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "sample = raw_datasets[\"train\"][12]\n",
    "inputs = tokenizer(sample[\"inputs\"])\n",
    "targets = tokenizer(sample[\"targets\"])\n",
    "\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1fd68e",
   "metadata": {
    "id": "q_WFIeD0BCXt"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "sample = raw_datasets[\"train\"][12]\n",
    "inputs = tokenizer(sample[\"inputs\"])\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(sample[\"targets\"])\n",
    "\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea376725",
   "metadata": {
    "id": "CklEyOljBCXt"
   },
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"inputs\"], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"targets\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function, batched=True, remove_columns=[\"inputs\", \"targets\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6561c9e4",
   "metadata": {
    "id": "LBOCwERbBCXu"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
