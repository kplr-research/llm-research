{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f8ad00",
   "metadata": {
    "id": "-5cJZlrJWWgt"
   },
   "source": [
    "## Comment entraîner un nouveau tokenizer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98dd67",
   "metadata": {
    "id": "z7aupZHqDgh8"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "  'huggingface-course/bert-base-uncased-tokenizer-without-normalizer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6049e1",
   "metadata": {
    "id": "kDOt-QkLDgh9"
   },
   "outputs": [],
   "source": [
    "text = \"here is a sentence adapted to our tokenizer\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba1721",
   "metadata": {
    "id": "9yX6w945Dgh9"
   },
   "outputs": [],
   "source": [
    "text = \"এই বাক্যটি আমাদের টোকেনাইজারের উপযুক্ত নয়\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ecba4",
   "metadata": {
    "id": "fUzEuvrdDgh-"
   },
   "outputs": [],
   "source": [
    "text = \"this tokenizer does not know àccënts and CAPITAL LETTERS\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ddc856",
   "metadata": {
    "id": "g94EJygVDgh-"
   },
   "outputs": [],
   "source": [
    "text = \"the medical vocabulary is divided into many sub-token: paracetamol, phrayngitis\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e712f9",
   "metadata": {
    "id": "uKrF89SPDgh-"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edef56b",
   "metadata": {
    "id": "oR0wKlgQDgh_"
   },
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c5245",
   "metadata": {
    "id": "kaWIhwfBDgh_"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "training_corpus = get_training_corpus()\n",
    "new_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\n",
    "new_tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de6400",
   "metadata": {
    "id": "u0fx0fMoDgh_"
   },
   "outputs": [],
   "source": [
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "\n",
    "print(old_tokenizer.tokenize(example))\n",
    "print(new_tokenizer.tokenize(example))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
