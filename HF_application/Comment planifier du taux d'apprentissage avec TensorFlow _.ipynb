{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf72bad",
   "metadata": {
    "id": "uC0fa9mITNiM"
   },
   "source": [
    "## Comment Faire du Finetuning avec TensorFlow ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d510311",
   "metadata": {
    "id": "qr6tiF9wCsGe"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    encoded = tokenizer(\n",
    "        dataset[\"sentence1\"],\n",
    "        dataset[\"sentence2\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return encoded.data\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_dataset, batched=True)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=8)\n",
    "\n",
    "validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093a791b",
   "metadata": {
    "id": "TBnM64bRCsGe"
   },
   "outputs": [],
   "source": [
    "next(iter(train_dataset))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543f3ef",
   "metadata": {
    "id": "Wbxp6BvHCsGe"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = 'bert-base-cased'\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04647a03",
   "metadata": {
    "id": "Jm8QLxZECsGe"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=3\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
