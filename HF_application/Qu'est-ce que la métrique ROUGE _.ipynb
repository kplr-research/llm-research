{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037a38ac",
   "metadata": {
    "id": "8VGYn1JtahDo"
   },
   "source": [
    "## Comment faire le traitement des données pour le résumé?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc8dc8",
   "metadata": {
    "id": "JMU93s46BVPr"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"xsum\")\n",
    "raw_datasets = raw_datasets.remove_columns([\"id\"])\n",
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a4601",
   "metadata": {
    "id": "YZJ1D-IkBVPr"
   },
   "outputs": [],
   "source": [
    "print(raw_datasets[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da0fad",
   "metadata": {
    "id": "tdDc9PjFBVPr"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "sample = raw_datasets[\"train\"][1]\n",
    "inputs = tokenizer(sample[\"document\"])\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(sample[\"summary\"])\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(targets[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b2d10",
   "metadata": {
    "id": "8ayGYps-BVPs"
   },
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"document\"], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function, batched=True, remove_columns=[\"document\", \"summary\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b707086",
   "metadata": {
    "id": "ieyqH74qBVPs"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
