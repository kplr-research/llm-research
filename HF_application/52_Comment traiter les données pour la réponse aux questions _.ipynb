{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf99559",
   "metadata": {
    "id": "OjH5arAfa65U"
   },
   "source": [
    "## Comment utiliser une fonction de perte personnalis√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b7072",
   "metadata": {
    "id": "uYy0amQSCCEg"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"huggingface-course/codeparrot-ds\")\n",
    "\n",
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"plt\",\n",
    "    \"pd\",\n",
    "    \"sk\",\n",
    "    \"fit\",\n",
    "    \"predict\",\n",
    "    \" plt\",\n",
    "    \" pd\",\n",
    "    \" sk\",\n",
    "    \" fit\",\n",
    "    \" predict\",\n",
    "]:\n",
    "    ids = tokenizer([keyword]).input_ids[0]\n",
    "    keytoken_ids.append(ids[0])\n",
    "\n",
    "batch = tokenizer([\"import numpy as np\"], return_tensors=\"pt\")\n",
    "model = accelerator.prepare(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640bbab8",
   "metadata": {
    "id": "9Ks20Pj8CCEh"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "\n",
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # Resize and average loss per sample\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    # Calculate and scale weighting\n",
    "    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
    "        axis=[0, 2]\n",
    "    )\n",
    "    weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    weighted_loss = (loss_per_sample * weights).mean()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05efddd",
   "metadata": {
    "id": "DKb_k54cCCEh"
   },
   "outputs": [],
   "source": [
    "logits = model(batch[\"input_ids\"]).logits\n",
    "loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "accelerator.backward(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae0d9f",
   "metadata": {
    "id": "pISweOBLCCEi"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        input_ids = inputs.get(\"input_ids\")\n",
    "        outputs = model(input_ids)\n",
    "        loss = keytoken_weighted_loss(input_ids, outputs.logits, keytoken_ids)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
