{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1184d5d7",
   "metadata": {
    "id": "MtJBahZbb2p-"
   },
   "source": [
    "## Comment g√©rer les erreurs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e17f1",
   "metadata": {
    "id": "RyXlGctBb6c6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed77652",
   "metadata": {
    "id": "sCuDhbHUC2Mf"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f4e3b8",
   "metadata": {
    "id": "tTmqk09BC2Mg"
   },
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c17c9c",
   "metadata": {
    "id": "-wNZarkDC2Mg"
   },
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c7c30",
   "metadata": {
    "id": "ba9jREz2C2Mg"
   },
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e744a4f",
   "metadata": {
    "id": "q20eCgvyC2Mg"
   },
   "outputs": [],
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d2400",
   "metadata": {
    "id": "jdKxtEuMC2Mh"
   },
   "outputs": [],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3658ce",
   "metadata": {
    "id": "wIPYr2B5C2Mh"
   },
   "outputs": [],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc568729",
   "metadata": {
    "id": "9QEHAy7lC2Mh"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1edfa5",
   "metadata": {
    "id": "Kybj3cuWC2Mh"
   },
   "outputs": [],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf9fe3",
   "metadata": {
    "id": "UNcnKcnZC2Mh"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ebbc8",
   "metadata": {
    "id": "kt1vB0D0C2Mh"
   },
   "outputs": [],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d1bb41",
   "metadata": {
    "id": "q6gvNM_RC2Mi"
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c3961",
   "metadata": {
    "id": "kgZ37vBpC2Mi"
   },
   "outputs": [],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c84b9e",
   "metadata": {
    "id": "LtI0Wvs3C2Mi"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b9199e",
   "metadata": {
    "id": "XTlD3PPLC2Mi"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4bee0f",
   "metadata": {
    "id": "AFRqewNFC2Mi"
   },
   "outputs": [],
   "source": [
    "features = [tokenized_datasets[\"train\"][i] for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d4f74a",
   "metadata": {
    "id": "ygjKqm3vC2Mi"
   },
   "outputs": [],
   "source": [
    "batch = tokenizer.pad(features, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819d175",
   "metadata": {
    "id": "NpygICUqC2Mi"
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
