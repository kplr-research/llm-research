{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a916add",
   "metadata": {
    "id": "2bBPKg79at59"
   },
   "source": [
    "## Comment effectuer un traitement des données pour la modélisation du langage causal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c1f610",
   "metadata": {
    "id": "dWtOX2-iB4yC"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"train\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,\n",
    "        \"valid\": ds_valid,\n",
    "    }\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"huggingface-course/codeparrot-ds\")\n",
    "batch = tokenizer([\"import numpy as np\"], return_tensors=\"pt\")\n",
    "\n",
    "text = \"import numpy as np\\n\"*20\n",
    "context_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ef7cb",
   "metadata": {
    "id": "1v8wNHJTB4yC"
   },
   "outputs": [],
   "source": [
    "outputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=16,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c4abfd",
   "metadata": {
    "id": "FDlrEzCJB4yC"
   },
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c4a61",
   "metadata": {
    "id": "QKz8QkZ8B4yD"
   },
   "outputs": [],
   "source": [
    "output = model(input_ids=batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "loss = output.loss"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
