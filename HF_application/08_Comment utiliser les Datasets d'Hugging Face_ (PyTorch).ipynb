{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8384e677",
   "metadata": {
    "id": "IhtMyZ1xRYv6"
   },
   "source": [
    "## Comment grouper les entr√©es ? (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdcd88f",
   "metadata": {
    "id": "xk1Yw16mRflS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ded9ad",
   "metadata": {
    "id": "m3Op8t5_AzG4"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sentences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this.\",\n",
    "]\n",
    "tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "print(ids[0])\n",
    "print(ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f28e91",
   "metadata": {
    "id": "iihZvFwSAzG5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ids = [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],\n",
    "       [1045, 5223, 2023, 1012]]\n",
    "\n",
    "input_ids = torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62dacd3",
   "metadata": {
    "id": "vjfGQl6zAzG5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ids = [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],\n",
    "       [1045, 5223, 2023, 1012,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0]]\n",
    "\n",
    "input_ids = torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78036e88",
   "metadata": {
    "id": "C_cdClNVAzG5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2bdc0",
   "metadata": {
    "id": "5lLoBsvXAzG6"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "ids1 = torch.tensor(\n",
    "    [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]]\n",
    ")\n",
    "ids2 = torch.tensor([[1045, 5223, 2023, 1012]])\n",
    "all_ids = torch.tensor(\n",
    "    [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],\n",
    "     [1045, 5223, 2023, 1012,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0]]\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "print(model(ids1).logits)\n",
    "print(model(ids2).logits)\n",
    "print(model(all_ids).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e53f86",
   "metadata": {
    "id": "iK4pRvy5AzG6"
   },
   "outputs": [],
   "source": [
    "all_ids = torch.tensor(\n",
    "    [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],\n",
    "     [1045, 5223, 2023, 1012,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0]]\n",
    ")\n",
    "attention_mask = torch.tensor(\n",
    "    [[   1,    1,    1,    1,    1,    1,    1,     1,     1,    1,    1,    1,    1,    1],\n",
    "     [   1,    1,    1,    1,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f5986",
   "metadata": {
    "id": "e2q5fJogAzG6"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "output1 = model(ids1)\n",
    "output2 = model(ids2)\n",
    "print(output1.logits)\n",
    "print(output2.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6ff09",
   "metadata": {
    "id": "xRvaI6XLAzG7"
   },
   "outputs": [],
   "source": [
    "output = model(all_ids, attention_mask=attention_mask)\n",
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74642e52",
   "metadata": {
    "id": "WBgEYN54AzG7"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sentences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this.\",\n",
    "]\n",
    "print(tokenizer(sentences, padding=True))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
